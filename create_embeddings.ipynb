{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOG8+2J6/EeDdqIAQNkqbVl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shannn1/goodRAG/blob/main/create_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## embeddings"
      ],
      "metadata": {
        "id": "WF4rlnrcPsO6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3atrtfljc6D"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "import torch\n",
        "import os\n",
        "from torch.cuda.amp import autocast\n",
        "from huggingface_hub import login\n",
        "from datasets import DatasetDict\n",
        "\n",
        "dataset = load_dataset(\"lighteval/natural_questions_clean\")\n",
        "train_data = dataset[\"train\"].select_columns([\"id\", \"title\", \"document\"])\n",
        "test_data = dataset[\"validation\"].select_columns([\"id\", \"title\", \"document\"])\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "def compute_embeddings(documents, ctx_encoder, ctx_tokenizer):\n",
        "    max_length = ctx_tokenizer.model_max_length\n",
        "    truncated_documents = {\n",
        "        \"title\": [t[:max_length] for t in documents[\"title\"]],\n",
        "        \"document\": [d[:max_length] for d in documents[\"document\"]],\n",
        "    }\n",
        "    inputs = ctx_tokenizer(\n",
        "        truncated_documents[\"title\"],\n",
        "        truncated_documents[\"document\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"longest\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with autocast():\n",
        "            embeddings = ctx_encoder(input_ids, return_dict=True).pooler_output\n",
        "\n",
        "    del input_ids, inputs\n",
        "    torch.cuda.empty_cache()\n",
        "    return embeddings.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def process_and_save(data, output_dir, batch_size, split_name):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    num_shards = (len(data) + batch_size - 1) // batch_size\n",
        "    for shard_index in range(num_shards):\n",
        "        start = shard_index * batch_size\n",
        "        end = min(start + batch_size, len(data))\n",
        "        batch = data.select(range(start, end))\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                print(f\"Processing shard {shard_index + 1}/{num_shards} for {split_name}.\")\n",
        "                print(f\"Remaining GPU memory: {torch.cuda.memory_allocated(device) / 1e6:.2f} MB\")\n",
        "\n",
        "                batch_with_embeddings = batch.map(\n",
        "                    lambda b: {\"embeddings\": compute_embeddings(b, ctx_encoder, ctx_tokenizer).tolist()},\n",
        "                    batched=True,\n",
        "                    batch_size=batch_size,\n",
        "                )\n",
        "                batch_with_embeddings.save_to_disk(os.path.join(output_dir, f\"{split_name}_batch_{shard_index}.dataset\"))\n",
        "                del batch_with_embeddings\n",
        "                break\n",
        "            except RuntimeError as e:\n",
        "                if \"CUDA out of memory\" in str(e):\n",
        "                    batch_size = max(batch_size // 2, 1)\n",
        "                    print(f\"Reduced batch size to {batch_size} due to OOM.\")\n",
        "                else:\n",
        "                    raise e\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error while processing batch: {e}\")\n",
        "                raise e\n",
        "\n",
        "\n",
        "def upload_batches_to_hub(batches, dataset_name, token):\n",
        "    login(token=token)\n",
        "\n",
        "    try:\n",
        "        existing_dataset = DatasetDict.load_from_hub(dataset_name)\n",
        "        print(f\"Loaded existing dataset '{dataset_name}' from Hugging Face Hub.\")\n",
        "    except FileNotFoundError:\n",
        "        existing_dataset = None\n",
        "        print(f\"Dataset '{dataset_name}' not found on Hugging Face Hub. Creating a new one.\")\n",
        "\n",
        "    for i, batch in enumerate(batches):\n",
        "        try:\n",
        "            if existing_dataset is None:\n",
        "                dataset_dict = DatasetDict({\"train\": batch})\n",
        "                dataset_dict.push_to_hub(dataset_name)\n",
        "                print(f\"Uploaded initial batch {i} to Hugging Face Hub as '{dataset_name}'.\")\n",
        "            else:\n",
        "                combined_dataset = DatasetDict({\n",
        "                    \"train\": concatenate_datasets([existing_dataset[\"train\"], batch])\n",
        "                })\n",
        "                combined_dataset.push_to_hub(dataset_name)\n",
        "                print(f\"Appended batch {i} to '{dataset_name}' on Hugging Face Hub.\")\n",
        "                existing_dataset = combined_dataset\n",
        "        except Exception as e:\n",
        "            print(f\"Error while uploading batch {i} to Hugging Face Hub: {e}\")\n",
        "            raise e\n",
        "\n",
        "train_batch_size = 256\n",
        "test_batch_size = 256\n",
        "\n",
        "process_and_save(train_data, output_dir=\"./train_batches\", batch_size=train_batch_size, split_name=\"train\")\n",
        "process_and_save(test_data, output_dir=\"./test_batches\", batch_size=test_batch_size, split_name=\"test\")\n",
        "\n",
        "train_batches = [\n",
        "    Dataset.load_from_disk(os.path.join(\"./train_batches\", f))\n",
        "    for f in os.listdir(\"./train_batches\") if f.endswith(\".dataset\")\n",
        "]\n",
        "test_batches = [\n",
        "    Dataset.load_from_disk(os.path.join(\"./test_batches\", f))\n",
        "    for f in os.listdir(\"./test_batches\") if f.endswith(\".dataset\")\n",
        "]\n",
        "final_dataset = concatenate_datasets(train_batches + test_batches)\n",
        "\n",
        "final_dataset.save_to_disk(\"./final_knowledge_base\")\n",
        "print(\"Final knowledge base saved to './final_knowledge_base'.\")\n",
        "\n",
        "upload_batches_to_hub(train_batches + test_batches, \"knowledge_base_genai\", token=\"change to your token\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tips for using AWS"
      ],
      "metadata": {
        "id": "cLk7MyH3P4Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload file\n",
        "scp -i \"genai.pem\" faiss_create.py ubuntu@ec2-35-153-208-211.compute-1.amazonaws.com"
      ],
      "metadata": {
        "id": "DfNBwjzZ1gNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sftp -i genai.pem ubuntu@ec2-35-153-208-211.compute-1.amazonaws.com"
      ],
      "metadata": {
        "id": "RF30aK5H_Uju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "put faiss_create.py"
      ],
      "metadata": {
        "id": "T6dkI_3E_YCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to the instance\n",
        "ssh -i genai.pem ubuntu@ec2-35-153-208-211.compute-1.amazonaws.com"
      ],
      "metadata": {
        "id": "3VUHQYdd2BT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate and use python env\n",
        "source ~/env/bin/activate\n",
        "python ~/try.py"
      ],
      "metadata": {
        "id": "061-FUn12cbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deleted folder\n",
        "rm -rf /path/to/your/folder"
      ],
      "metadata": {
        "id": "J9yriCVdO1gP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}