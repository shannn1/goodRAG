{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip uninstall -y faiss faiss-cpu faiss-gpu\n",
        "!pip install faiss-cpu\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDkpvk7iNbCv",
        "outputId": "4c311029-ee0a-46bb-d78e-cf863db66a47",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "\u001b[33mWARNING: Skipping faiss as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: faiss-cpu 1.9.0.post1\n",
            "Uninstalling faiss-cpu-1.9.0.post1:\n",
            "  Successfully uninstalled faiss-cpu-1.9.0.post1\n",
            "\u001b[33mWARNING: Skipping faiss-gpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting faiss-cpu\n",
            "  Using cached faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Using cached faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.9.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "!rm -r /content/goodRAG\n",
        "os.system('git clone https://github.com/shannn1/goodRAG.git')\n",
        "sys.path.append('/content/goodRAG')"
      ],
      "metadata": {
        "id": "UsL0Oa9hek-T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import faiss\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from modeling_rag import RagRetriever, RagSequenceForGeneration, RagConfig\n",
        "from tokenization_rag import RagTokenizer\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DPRQuestionEncoder,\n",
        "    DPRQuestionEncoderTokenizer,\n",
        "    AdamW,\n",
        "    get_scheduler\n",
        ")\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast"
      ],
      "metadata": {
        "id": "yEmzPsJCd4_G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_base = load_dataset(\"Shannnh/knowledge_base_genai\")\n",
        "knowledge_base = knowledge_base['train']\n",
        "knowledge_base = knowledge_base.rename_column(\"document\", \"text\")\n",
        "output_path = \"./knowledge_base\"\n",
        "knowledge_base.save_to_disk(output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "f676f92187a844b4bdd2d4a40d181044",
            "a3b85bfadbe34359a9c066a70ccccf2e",
            "7079f5f07aa242739498c86bbb82c4c0",
            "cd5abb794fdb4958b638bac1709ae085",
            "ad8fa5ef859644c98ab0d8237fbe5e7e",
            "a11589e505dd4b0c8f6ea9c5492c9859",
            "bb750d17a3404be48e5d4383f04bf82c",
            "4fc94ecf2d6d47bf9c2f4ba788ab8e86",
            "1510ed1d80cf47f290d241e3c483b4e1",
            "58b85f1b052c4424ab5852312d0c7924",
            "01b86176ac6744c38ee9d34ae6f6a45a"
          ]
        },
        "id": "dAO0cP7madTF",
        "outputId": "3c797e77-1046-4daf-f604-83c3c8e12b3f",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/8 shards):   0%|          | 0/78529 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f676f92187a844b4bdd2d4a40d181044"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "generator_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "\n",
        "indexPath = \"./faiss_index\"\n",
        "passagesPath = \"./knowledge_base\"\n",
        "\n",
        "rag_config = RagConfig.from_question_encoder_generator_configs(\n",
        "    question_encoder.config,\n",
        "    generator.config,\n",
        "    index_name=\"custom\",\n",
        "    n_docs=5,\n",
        "    index_path=indexPath,\n",
        "    passages_path=passagesPath,\n",
        ")\n",
        "\n",
        "retriever = RagRetriever(\n",
        "    config=rag_config,\n",
        "    question_encoder_tokenizer=question_encoder_tokenizer,\n",
        "    generator_tokenizer=generator_tokenizer,\n",
        ")\n",
        "\n",
        "model = RagSequenceForGeneration(\n",
        "    config=rag_config,\n",
        "    question_encoder=question_encoder,\n",
        "    generator=generator,\n",
        "    retriever=retriever,\n",
        ")\n",
        "\n",
        "retriever.config.index_path = rag_config.index_path\n",
        "retriever.config.passages_path = rag_config.passages_path\n",
        "retriever.config.generator = rag_config.generator\n",
        "retriever.config.question_encoder = rag_config.question_encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8UZ7wtAJWiw",
        "outputId": "e9e4cecb-6f66-49a3-90bf-3cab49d86b27"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
        "print(f\"Trainable Parameters: {trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTscaieo51cW",
        "outputId": "e5430804-8f3f-40a2-a5de-0a826a15e9d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable Parameters: 515182080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if \"decoder.layers.6\" not in name and \"decoder.layers.7\" not in name and \"decoder.layers.8\" not in name and \"decoder.layers.9\" not in name and \"decoder.layers.10\" not in name and \"decoder.layers.11\" not in name:  # train only last 2 layers\n",
        "        param.requires_grad = False\n",
        "\n",
        "trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
        "print(\"Trainable Parameters:\", trainable_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naMrHkRH7gL1",
        "outputId": "65f44df4-7687-4834-f0c5-8b58b30f580f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable Parameters: ['rag.generator.model.decoder.layers.6.self_attn.k_proj.weight', 'rag.generator.model.decoder.layers.6.self_attn.k_proj.bias', 'rag.generator.model.decoder.layers.6.self_attn.v_proj.weight', 'rag.generator.model.decoder.layers.6.self_attn.v_proj.bias', 'rag.generator.model.decoder.layers.6.self_attn.q_proj.weight', 'rag.generator.model.decoder.layers.6.self_attn.q_proj.bias', 'rag.generator.model.decoder.layers.6.self_attn.out_proj.weight', 'rag.generator.model.decoder.layers.6.self_attn.out_proj.bias', 'rag.generator.model.decoder.layers.6.self_attn_layer_norm.weight', 'rag.generator.model.decoder.layers.6.self_attn_layer_norm.bias', 'rag.generator.model.decoder.layers.6.encoder_attn.k_proj.weight', 'rag.generator.model.decoder.layers.6.encoder_attn.k_proj.bias', 'rag.generator.model.decoder.layers.6.encoder_attn.v_proj.weight', 'rag.generator.model.decoder.layers.6.encoder_attn.v_proj.bias', 'rag.generator.model.decoder.layers.6.encoder_attn.q_proj.weight', 'rag.generator.model.decoder.layers.6.encoder_attn.q_proj.bias', 'rag.generator.model.decoder.layers.6.encoder_attn.out_proj.weight', 'rag.generator.model.decoder.layers.6.encoder_attn.out_proj.bias', 'rag.generator.model.decoder.layers.6.encoder_attn_layer_norm.weight', 'rag.generator.model.decoder.layers.6.encoder_attn_layer_norm.bias', 'rag.generator.model.decoder.layers.6.fc1.weight', 'rag.generator.model.decoder.layers.6.fc1.bias', 'rag.generator.model.decoder.layers.6.fc2.weight', 'rag.generator.model.decoder.layers.6.fc2.bias', 'rag.generator.model.decoder.layers.6.final_layer_norm.weight', 'rag.generator.model.decoder.layers.6.final_layer_norm.bias', 'rag.generator.model.decoder.layers.7.self_attn.k_proj.weight', 'rag.generator.model.decoder.layers.7.self_attn.k_proj.bias', 'rag.generator.model.decoder.layers.7.self_attn.v_proj.weight', 'rag.generator.model.decoder.layers.7.self_attn.v_proj.bias', 'rag.generator.model.decoder.layers.7.self_attn.q_proj.weight', 'rag.generator.model.decoder.layers.7.self_attn.q_proj.bias', 'rag.generator.model.decoder.layers.7.self_attn.out_proj.weight', 'rag.generator.model.decoder.layers.7.self_attn.out_proj.bias', 'rag.generator.model.decoder.layers.7.self_attn_layer_norm.weight', 'rag.generator.model.decoder.layers.7.self_attn_layer_norm.bias', 'rag.generator.model.decoder.layers.7.encoder_attn.k_proj.weight', 'rag.generator.model.decoder.layers.7.encoder_attn.k_proj.bias', 'rag.generator.model.decoder.layers.7.encoder_attn.v_proj.weight', 'rag.generator.model.decoder.layers.7.encoder_attn.v_proj.bias', 'rag.generator.model.decoder.layers.7.encoder_attn.q_proj.weight', 'rag.generator.model.decoder.layers.7.encoder_attn.q_proj.bias', 'rag.generator.model.decoder.layers.7.encoder_attn.out_proj.weight', 'rag.generator.model.decoder.layers.7.encoder_attn.out_proj.bias', 'rag.generator.model.decoder.layers.7.encoder_attn_layer_norm.weight', 'rag.generator.model.decoder.layers.7.encoder_attn_layer_norm.bias', 'rag.generator.model.decoder.layers.7.fc1.weight', 'rag.generator.model.decoder.layers.7.fc1.bias', 'rag.generator.model.decoder.layers.7.fc2.weight', 'rag.generator.model.decoder.layers.7.fc2.bias', 'rag.generator.model.decoder.layers.7.final_layer_norm.weight', 'rag.generator.model.decoder.layers.7.final_layer_norm.bias', 'rag.generator.model.decoder.layers.8.self_attn.k_proj.weight', 'rag.generator.model.decoder.layers.8.self_attn.k_proj.bias', 'rag.generator.model.decoder.layers.8.self_attn.v_proj.weight', 'rag.generator.model.decoder.layers.8.self_attn.v_proj.bias', 'rag.generator.model.decoder.layers.8.self_attn.q_proj.weight', 'rag.generator.model.decoder.layers.8.self_attn.q_proj.bias', 'rag.generator.model.decoder.layers.8.self_attn.out_proj.weight', 'rag.generator.model.decoder.layers.8.self_attn.out_proj.bias', 'rag.generator.model.decoder.layers.8.self_attn_layer_norm.weight', 'rag.generator.model.decoder.layers.8.self_attn_layer_norm.bias', 'rag.generator.model.decoder.layers.8.encoder_attn.k_proj.weight', 'rag.generator.model.decoder.layers.8.encoder_attn.k_proj.bias', 'rag.generator.model.decoder.layers.8.encoder_attn.v_proj.weight', 'rag.generator.model.decoder.layers.8.encoder_attn.v_proj.bias', 'rag.generator.model.decoder.layers.8.encoder_attn.q_proj.weight', 'rag.generator.model.decoder.layers.8.encoder_attn.q_proj.bias', 'rag.generator.model.decoder.layers.8.encoder_attn.out_proj.weight', 'rag.generator.model.decoder.layers.8.encoder_attn.out_proj.bias', 'rag.generator.model.decoder.layers.8.encoder_attn_layer_norm.weight', 'rag.generator.model.decoder.layers.8.encoder_attn_layer_norm.bias', 'rag.generator.model.decoder.layers.8.fc1.weight', 'rag.generator.model.decoder.layers.8.fc1.bias', 'rag.generator.model.decoder.layers.8.fc2.weight', 'rag.generator.model.decoder.layers.8.fc2.bias', 'rag.generator.model.decoder.layers.8.final_layer_norm.weight', 'rag.generator.model.decoder.layers.8.final_layer_norm.bias', 'rag.generator.model.decoder.layers.9.self_attn.k_proj.weight', 'rag.generator.model.decoder.layers.9.self_attn.k_proj.bias', 'rag.generator.model.decoder.layers.9.self_attn.v_proj.weight', 'rag.generator.model.decoder.layers.9.self_attn.v_proj.bias', 'rag.generator.model.decoder.layers.9.self_attn.q_proj.weight', 'rag.generator.model.decoder.layers.9.self_attn.q_proj.bias', 'rag.generator.model.decoder.layers.9.self_attn.out_proj.weight', 'rag.generator.model.decoder.layers.9.self_attn.out_proj.bias', 'rag.generator.model.decoder.layers.9.self_attn_layer_norm.weight', 'rag.generator.model.decoder.layers.9.self_attn_layer_norm.bias', 'rag.generator.model.decoder.layers.9.encoder_attn.k_proj.weight', 'rag.generator.model.decoder.layers.9.encoder_attn.k_proj.bias', 'rag.generator.model.decoder.layers.9.encoder_attn.v_proj.weight', 'rag.generator.model.decoder.layers.9.encoder_attn.v_proj.bias', 'rag.generator.model.decoder.layers.9.encoder_attn.q_proj.weight', 'rag.generator.model.decoder.layers.9.encoder_attn.q_proj.bias', 'rag.generator.model.decoder.layers.9.encoder_attn.out_proj.weight', 'rag.generator.model.decoder.layers.9.encoder_attn.out_proj.bias', 'rag.generator.model.decoder.layers.9.encoder_attn_layer_norm.weight', 'rag.generator.model.decoder.layers.9.encoder_attn_layer_norm.bias', 'rag.generator.model.decoder.layers.9.fc1.weight', 'rag.generator.model.decoder.layers.9.fc1.bias', 'rag.generator.model.decoder.layers.9.fc2.weight', 'rag.generator.model.decoder.layers.9.fc2.bias', 'rag.generator.model.decoder.layers.9.final_layer_norm.weight', 'rag.generator.model.decoder.layers.9.final_layer_norm.bias', 'rag.generator.model.decoder.layers.10.self_attn.k_proj.weight', 'rag.generator.model.decoder.layers.10.self_attn.k_proj.bias', 'rag.generator.model.decoder.layers.10.self_attn.v_proj.weight', 'rag.generator.model.decoder.layers.10.self_attn.v_proj.bias', 'rag.generator.model.decoder.layers.10.self_attn.q_proj.weight', 'rag.generator.model.decoder.layers.10.self_attn.q_proj.bias', 'rag.generator.model.decoder.layers.10.self_attn.out_proj.weight', 'rag.generator.model.decoder.layers.10.self_attn.out_proj.bias', 'rag.generator.model.decoder.layers.10.self_attn_layer_norm.weight', 'rag.generator.model.decoder.layers.10.self_attn_layer_norm.bias', 'rag.generator.model.decoder.layers.10.encoder_attn.k_proj.weight', 'rag.generator.model.decoder.layers.10.encoder_attn.k_proj.bias', 'rag.generator.model.decoder.layers.10.encoder_attn.v_proj.weight', 'rag.generator.model.decoder.layers.10.encoder_attn.v_proj.bias', 'rag.generator.model.decoder.layers.10.encoder_attn.q_proj.weight', 'rag.generator.model.decoder.layers.10.encoder_attn.q_proj.bias', 'rag.generator.model.decoder.layers.10.encoder_attn.out_proj.weight', 'rag.generator.model.decoder.layers.10.encoder_attn.out_proj.bias', 'rag.generator.model.decoder.layers.10.encoder_attn_layer_norm.weight', 'rag.generator.model.decoder.layers.10.encoder_attn_layer_norm.bias', 'rag.generator.model.decoder.layers.10.fc1.weight', 'rag.generator.model.decoder.layers.10.fc1.bias', 'rag.generator.model.decoder.layers.10.fc2.weight', 'rag.generator.model.decoder.layers.10.fc2.bias', 'rag.generator.model.decoder.layers.10.final_layer_norm.weight', 'rag.generator.model.decoder.layers.10.final_layer_norm.bias', 'rag.generator.model.decoder.layers.11.self_attn.k_proj.weight', 'rag.generator.model.decoder.layers.11.self_attn.k_proj.bias', 'rag.generator.model.decoder.layers.11.self_attn.v_proj.weight', 'rag.generator.model.decoder.layers.11.self_attn.v_proj.bias', 'rag.generator.model.decoder.layers.11.self_attn.q_proj.weight', 'rag.generator.model.decoder.layers.11.self_attn.q_proj.bias', 'rag.generator.model.decoder.layers.11.self_attn.out_proj.weight', 'rag.generator.model.decoder.layers.11.self_attn.out_proj.bias', 'rag.generator.model.decoder.layers.11.self_attn_layer_norm.weight', 'rag.generator.model.decoder.layers.11.self_attn_layer_norm.bias', 'rag.generator.model.decoder.layers.11.encoder_attn.k_proj.weight', 'rag.generator.model.decoder.layers.11.encoder_attn.k_proj.bias', 'rag.generator.model.decoder.layers.11.encoder_attn.v_proj.weight', 'rag.generator.model.decoder.layers.11.encoder_attn.v_proj.bias', 'rag.generator.model.decoder.layers.11.encoder_attn.q_proj.weight', 'rag.generator.model.decoder.layers.11.encoder_attn.q_proj.bias', 'rag.generator.model.decoder.layers.11.encoder_attn.out_proj.weight', 'rag.generator.model.decoder.layers.11.encoder_attn.out_proj.bias', 'rag.generator.model.decoder.layers.11.encoder_attn_layer_norm.weight', 'rag.generator.model.decoder.layers.11.encoder_attn_layer_norm.bias', 'rag.generator.model.decoder.layers.11.fc1.weight', 'rag.generator.model.decoder.layers.11.fc1.bias', 'rag.generator.model.decoder.layers.11.fc2.weight', 'rag.generator.model.decoder.layers.11.fc2.bias', 'rag.generator.model.decoder.layers.11.final_layer_norm.weight', 'rag.generator.model.decoder.layers.11.final_layer_norm.bias']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert model.config.index_path == retriever.config.index_path, \"Index path mismatch!\"\n",
        "assert model.config.passages_path == retriever.config.passages_path, \"Passages path mismatch!\"\n",
        "assert model.config.generator == retriever.config.generator, \"Generator config mismatch!\"\n",
        "assert model.config.question_encoder == retriever.config.question_encoder, \"Question encoder config mismatch!\"\n",
        "print(\"All configurations are consistent!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXB1CFs1H7s6",
        "outputId": "0517984a-ad48-4938-f53a-7dac1899fc4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All configurations are consistent!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "wandb.init(project=\"rag_finetuning2\", name=\"rag2\")\n",
        "\n",
        "datasets = load_dataset(\"lighteval/natural_questions_clean\")\n",
        "\n",
        "# Split train and validation\n",
        "full_train_data = datasets['train']\n",
        "train_val_split = full_train_data.train_test_split(test_size=0.1, seed=42)\n",
        "train_data = train_val_split['train']\n",
        "validation_data = train_val_split['test']\n",
        "\n",
        "# Rename columns\n",
        "train_data = train_data.rename_column(\"short_answers\", \"answer\")\n",
        "validation_data = validation_data.rename_column(\"short_answers\", \"answer\")\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# device = 'cpu'\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "def preprocess_function(batch):\n",
        "    question = [q if isinstance(q, str) else \"\" for q in batch['question']]\n",
        "    answer = [a[0] if (isinstance(a, list) and len(a) > 0 and isinstance(a[0], str)) else \"\" for a in batch['answer']]\n",
        "    inputs = question_encoder_tokenizer(question, padding=\"max_length\", max_length = 512, truncation=True, return_tensors=\"pt\")\n",
        "    targets = generator_tokenizer(answer, padding=\"max_length\", max_length = 512, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": inputs[\"input_ids\"].tolist(),\n",
        "        \"attention_mask\": inputs[\"attention_mask\"].tolist(),\n",
        "        \"labels\": targets[\"input_ids\"].tolist(),\n",
        "    }\n",
        "\n",
        "train_dataset = train_data.map(preprocess_function, batched=True)\n",
        "validation_dataset = validation_data.map(preprocess_function, batched=True)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor([item[\"input_ids\"] for item in batch]),\n",
        "        \"attention_mask\": torch.tensor([item[\"attention_mask\"] for item in batch]),\n",
        "        \"labels\": torch.tensor([item[\"labels\"] for item in batch]),\n",
        "        \"question\": [item[\"question\"] for item in batch],\n",
        "        \"original_answer\": [item[\"answer\"] for item in batch],\n",
        "    }\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "epochs = 8\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_training_steps = len(train_dataloader) * epochs\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training loop\n",
        "    train_loss_total = 0.0\n",
        "    train_steps = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            if loss.dim() > 0:\n",
        "                loss = loss.mean()\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        train_loss_total += loss.item()\n",
        "        train_steps += 1\n",
        "\n",
        "    # Compute average training loss for the epoch\n",
        "    avg_train_loss = train_loss_total / train_steps if train_steps > 0 else 0.0\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_steps = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                val_loss += outputs.loss.mean().item()\n",
        "                val_steps += 1\n",
        "\n",
        "    avg_val_loss = val_loss / val_steps if val_steps > 0 else 0.0\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "    # Log epoch losses to wandb\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss_epoch\": avg_train_loss,\n",
        "        \"val_loss_epoch\": avg_val_loss\n",
        "    })\n",
        "\n",
        "    model.train()\n",
        "\n",
        "model.save_pretrained(\"finetuned_rag\")\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745,
          "referenced_widgets": [
            "44f71a4fce5340ecbde77f4e0d741d45",
            "f2d616c02cd64ae090d4222b52774b2b",
            "2cae42a802e14ecd92f3215e746a91a9",
            "5c1e038c1755486d989a86d5020a3c1d",
            "9f9c58abd0484e58b7f42ef8314400cb",
            "c301892394014e3793844bcf76fb01cf",
            "02883404970342449ea5ea6713174883",
            "1ee876bcdafb49eeb7a5125a3e602cf0",
            "71dd059aea6f474f8c61a9d9b37197fd",
            "9a91fee107f145cd98dbc47a081d174e",
            "d7627482cfce4db9a233ceb1deb66c5f"
          ]
        },
        "id": "lUyLXd6g9kjy",
        "outputId": "bba29bfe-2615-4daf-83c5-b882325e80f0",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlzheng2\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241212_234409-kmtlrl00</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lzheng2/rag_finetuning2/runs/kmtlrl00' target=\"_blank\">rag2</a></strong> to <a href='https://wandb.ai/lzheng2/rag_finetuning2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lzheng2/rag_finetuning2' target=\"_blank\">https://wandb.ai/lzheng2/rag_finetuning2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lzheng2/rag_finetuning2/runs/kmtlrl00' target=\"_blank\">https://wandb.ai/lzheng2/rag_finetuning2/runs/kmtlrl00</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44f71a4fce5340ecbde77f4e0d741d45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "<ipython-input-9-1101e4140f5b>:56: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-9-1101e4140f5b>:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-9-1101e4140f5b>:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 52.0580891167363, Validation Loss: 36.87710940645864\n",
            "Epoch 2, Train Loss: 38.07242177340907, Validation Loss: 30.8282016633203\n",
            "Epoch 3, Train Loss: 29.36421213846742, Validation Loss: 21.80650766636154\n",
            "Epoch 4, Train Loss: 20.00167305558406, Validation Loss: 20.966648106324303\n",
            "Epoch 5, Train Loss: 17.889120399074923, Validation Loss: 18.133521347543034\n",
            "Epoch 6, Train Loss: 16.68456099557555, Validation Loss: 17.370237055864155\n",
            "Epoch 7, Train Loss: 15.61680763535338, Validation Loss: 16.86936557345434\n",
            "Epoch 8, Train Loss: 15.039319696803531, Validation Loss: 17.90341077461493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 2}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▂▂▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▆▃▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>8</td></tr><tr><td>train_loss_epoch</td><td>15.03932</td></tr><tr><td>val_loss_epoch</td><td>17.90341</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rag2</strong> at: <a href='https://wandb.ai/lzheng2/rag_finetuning2/runs/kmtlrl00' target=\"_blank\">https://wandb.ai/lzheng2/rag_finetuning2/runs/kmtlrl00</a><br/> View project at: <a href='https://wandb.ai/lzheng2/rag_finetuning2' target=\"_blank\">https://wandb.ai/lzheng2/rag_finetuning2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241212_234409-kmtlrl00/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to the appropriate device and set to evaluation mode\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Prepare the test dataset\n",
        "test_data = datasets['validation']\n",
        "test_data = test_data.rename_column(\"short_answers\", \"answer\")\n",
        "test_dataset = test_data.map(preprocess_function, batched=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
        "        # Move input data to the appropriate device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        # Generate predictions\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=50,\n",
        "            num_beams=5\n",
        "        )\n",
        "\n",
        "        # Decode predictions\n",
        "        predicted_answers = generator_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Store results\n",
        "        for question, original_answer, predicted_answer in zip(batch[\"question\"], batch[\"original_answer\"], predicted_answers):\n",
        "            results.append({\n",
        "                \"question\": question,\n",
        "                \"original_answer\": original_answer,\n",
        "                \"predicted_answer\": predicted_answer\n",
        "            })\n",
        "\n",
        "# Save results to a file\n",
        "results_file = \"results.json\"\n",
        "with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"Test results saved to {results_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQuJMZhA305f",
        "outputId": "ea3ae92a-e5a3-4efe-f83f-d64a17ed0eea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 4289/4289 [38:45<00:00,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test results saved to results.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_f1(predicted, ground_truths):\n",
        "    if not isinstance(ground_truths, list):\n",
        "        ground_truths = [ground_truths]\n",
        "    max_f1 = 0.0\n",
        "    for ground_truth in ground_truths:\n",
        "        pred_tokens = predicted.split()\n",
        "        gt_tokens = ground_truth.split()\n",
        "        common = set(pred_tokens) & set(gt_tokens)\n",
        "        if len(common) == 0:\n",
        "            continue\n",
        "        precision = len(common) / len(pred_tokens)\n",
        "        recall = len(common) / len(gt_tokens)\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "        max_f1 = max(max_f1, f1)\n",
        "    return max_f1\n",
        "\n",
        "f1_scores = [\n",
        "    calculate_f1(item[\"predicted_answer\"], item[\"original_answer\"])\n",
        "    for item in results\n",
        "]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(f\"Average F1 Score: {average_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6uWHxgGXGco",
        "outputId": "d047869b-abdd-4454-8161-4e2b236fd919"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average F1 Score: 0.1874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_exact_match(predicted, ground_truths):\n",
        "    if not isinstance(ground_truths, list):\n",
        "        ground_truths = [ground_truths]\n",
        "    for ground_truth in ground_truths:\n",
        "        if predicted.strip() == ground_truth.strip():\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "em_scores = [\n",
        "    calculate_exact_match(item[\"predicted_answer\"], item[\"original_answer\"])\n",
        "    for item in results\n",
        "]\n",
        "average_em = sum(em_scores) / len(em_scores)\n",
        "print(f\"Exact Match Score: {average_em:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcxYxY5DXuNx",
        "outputId": "fdbb6ca6-30a6-41a0-f802-66fa07e2ece6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match Score: 0.0208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def calculate_q_bleu(predicted, ground_truths, question):\n",
        "    if isinstance(ground_truths, list):\n",
        "        ground_truths = \" \".join(ground_truths)\n",
        "\n",
        "    # Tokenize inputs\n",
        "    ref_tokens = ground_truths.split()\n",
        "    hyp_tokens = predicted.split()\n",
        "    question_tokens = question.split()\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    reference = [ref_tokens]\n",
        "    hypothesis = hyp_tokens\n",
        "    bleu_score = sentence_bleu(reference, hypothesis)\n",
        "\n",
        "    # Remove stopwords\n",
        "    important_ref_tokens = [token for token in ref_tokens if token.lower() not in stop_words]\n",
        "    important_hyp_tokens = [token for token in hyp_tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Key match ratio\n",
        "    key_match = len(set(important_ref_tokens) & set(important_hyp_tokens)) / max(len(set(important_ref_tokens)), 1)\n",
        "\n",
        "    # Question match ratio\n",
        "    question_match = len(set(question_tokens) & set(hyp_tokens)) / max(len(set(question_tokens)), 1)\n",
        "\n",
        "    # Weighted Q-BLEU score\n",
        "    q_bleu = 0.7 * bleu_score + 0.2 * key_match + 0.1 * question_match\n",
        "\n",
        "    return q_bleu\n",
        "\n",
        "q_bleu_scores = [\n",
        "    calculate_q_bleu(item[\"predicted_answer\"], item[\"original_answer\"], item[\"question\"])\n",
        "    for item in results\n",
        "]\n",
        "average_q_bleu = sum(q_bleu_scores) / len(q_bleu_scores)\n",
        "print(f\"Average Q-BLEU Score: {average_q_bleu:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCIeE0LOYQDa",
        "outputId": "3d544fa0-18d0-4477-9083-a4b7f699bc1c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Q-BLEU Score: 0.1363\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f676f92187a844b4bdd2d4a40d181044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3b85bfadbe34359a9c066a70ccccf2e",
              "IPY_MODEL_7079f5f07aa242739498c86bbb82c4c0",
              "IPY_MODEL_cd5abb794fdb4958b638bac1709ae085"
            ],
            "layout": "IPY_MODEL_ad8fa5ef859644c98ab0d8237fbe5e7e"
          }
        },
        "a3b85bfadbe34359a9c066a70ccccf2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a11589e505dd4b0c8f6ea9c5492c9859",
            "placeholder": "​",
            "style": "IPY_MODEL_bb750d17a3404be48e5d4383f04bf82c",
            "value": "Saving the dataset (8/8 shards): 100%"
          }
        },
        "7079f5f07aa242739498c86bbb82c4c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fc94ecf2d6d47bf9c2f4ba788ab8e86",
            "max": 78529,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1510ed1d80cf47f290d241e3c483b4e1",
            "value": 78529
          }
        },
        "cd5abb794fdb4958b638bac1709ae085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58b85f1b052c4424ab5852312d0c7924",
            "placeholder": "​",
            "style": "IPY_MODEL_01b86176ac6744c38ee9d34ae6f6a45a",
            "value": " 78529/78529 [00:25&lt;00:00, 5548.71 examples/s]"
          }
        },
        "ad8fa5ef859644c98ab0d8237fbe5e7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a11589e505dd4b0c8f6ea9c5492c9859": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb750d17a3404be48e5d4383f04bf82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fc94ecf2d6d47bf9c2f4ba788ab8e86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1510ed1d80cf47f290d241e3c483b4e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58b85f1b052c4424ab5852312d0c7924": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01b86176ac6744c38ee9d34ae6f6a45a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44f71a4fce5340ecbde77f4e0d741d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2d616c02cd64ae090d4222b52774b2b",
              "IPY_MODEL_2cae42a802e14ecd92f3215e746a91a9",
              "IPY_MODEL_5c1e038c1755486d989a86d5020a3c1d"
            ],
            "layout": "IPY_MODEL_9f9c58abd0484e58b7f42ef8314400cb"
          }
        },
        "f2d616c02cd64ae090d4222b52774b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c301892394014e3793844bcf76fb01cf",
            "placeholder": "​",
            "style": "IPY_MODEL_02883404970342449ea5ea6713174883",
            "value": "Map: 100%"
          }
        },
        "2cae42a802e14ecd92f3215e746a91a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ee876bcdafb49eeb7a5125a3e602cf0",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71dd059aea6f474f8c61a9d9b37197fd",
            "value": 20
          }
        },
        "5c1e038c1755486d989a86d5020a3c1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a91fee107f145cd98dbc47a081d174e",
            "placeholder": "​",
            "style": "IPY_MODEL_d7627482cfce4db9a233ceb1deb66c5f",
            "value": " 20/20 [00:00&lt;00:00, 242.61 examples/s]"
          }
        },
        "9f9c58abd0484e58b7f42ef8314400cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c301892394014e3793844bcf76fb01cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02883404970342449ea5ea6713174883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ee876bcdafb49eeb7a5125a3e602cf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71dd059aea6f474f8c61a9d9b37197fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a91fee107f145cd98dbc47a081d174e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7627482cfce4db9a233ceb1deb66c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
