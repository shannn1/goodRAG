{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjXLQ+kIMpvxEmr0G5fujD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shannn1/goodRAG/blob/main/embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC1wNkIP3Pc7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "import torch\n",
        "import os\n",
        "from torch.cuda.amp import autocast\n",
        "from huggingface_hub import login\n",
        "from datasets import DatasetDict\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def split_text(text: str, n=512, sep=\" \"):\n",
        "    words = text.split(sep)\n",
        "    passages = [sep.join(words[i : i + n]).strip() for i in range(0, len(words), n)]\n",
        "    return passages\n",
        "\n",
        "def split_documents(example, chunk_size=512):\n",
        "    titles, docs = example[\"title\"], example[\"document\"]\n",
        "    new_titles, new_docs = [], []\n",
        "    for title, doc in zip(titles, docs):\n",
        "        if doc is None:\n",
        "            continue\n",
        "        passages = split_text(doc, n=chunk_size)\n",
        "        for p in passages:\n",
        "            new_titles.append(title)\n",
        "            new_docs.append(p)\n",
        "    return {\"title\": new_titles, \"document\": new_docs}\n",
        "\n",
        "dataset = load_dataset(\"lighteval/natural_questions_clean\")\n",
        "\n",
        "train_data = dataset[\"train\"].select_columns([\"id\", \"title\", \"document\"])\n",
        "test_data = dataset[\"validation\"].select_columns([\"id\", \"title\", \"document\"])\n",
        "\n",
        "train_data = train_data.map(split_documents, batched=True, remove_columns=train_data.column_names)\n",
        "test_data = test_data.map(split_documents, batched=True, remove_columns=test_data.column_names)\n",
        "\n",
        "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "def compute_embeddings(documents, ctx_encoder, ctx_tokenizer):\n",
        "    inputs = ctx_tokenizer(\n",
        "        documents[\"title\"],\n",
        "        documents[\"document\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"longest\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with autocast():\n",
        "            embeddings = ctx_encoder(input_ids, return_dict=True).pooler_output\n",
        "\n",
        "    del input_ids, inputs\n",
        "    torch.cuda.empty_cache()\n",
        "    return embeddings.detach().cpu().numpy()\n",
        "\n",
        "def process_and_save(data, output_dir, batch_size, split_name):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    num_shards = (len(data) + batch_size - 1) // batch_size\n",
        "    for shard_index in range(num_shards):\n",
        "        start = shard_index * batch_size\n",
        "        end = min(start + batch_size, len(data))\n",
        "        batch = data.select(range(start, end))\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                print(f\"Processing shard {shard_index + 1}/{num_shards} for {split_name}.\")\n",
        "                print(f\"Currently allocated GPU memory: {torch.cuda.memory_allocated(device) / 1e6:.2f} MB\")\n",
        "\n",
        "                batch_with_embeddings = batch.map(\n",
        "                    lambda b: {\"embeddings\": compute_embeddings(b, ctx_encoder, ctx_tokenizer).tolist()},\n",
        "                    batched=True,\n",
        "                    batch_size=batch_size,\n",
        "                )\n",
        "                batch_with_embeddings.save_to_disk(os.path.join(output_dir, f\"{split_name}_batch_{shard_index}.dataset\"))\n",
        "                del batch_with_embeddings\n",
        "                break\n",
        "            except RuntimeError as e:\n",
        "                if \"CUDA out of memory\" in str(e):\n",
        "                    batch_size = max(batch_size // 2, 1)\n",
        "                    print(f\"Reduced batch size to {batch_size} due to OOM.\")\n",
        "                else:\n",
        "                    raise e\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error while processing batch: {e}\")\n",
        "                raise e\n",
        "\n",
        "def upload_batches_to_hub(batches, dataset_name, token, batch_merge_size=10):\n",
        "    login(token=token)\n",
        "\n",
        "    try:\n",
        "        existing_dataset = load_dataset(dataset_name)\n",
        "        print(f\"Loaded existing dataset '{dataset_name}' from Hugging Face Hub.\")\n",
        "    except FileNotFoundError:\n",
        "        existing_dataset = None\n",
        "        print(f\"Dataset '{dataset_name}' not found on Hugging Face Hub. Creating a new one.\")\n",
        "\n",
        "    for i in range(0, len(batches), batch_merge_size):\n",
        "        merged_batches = concatenate_datasets(batches[i:i + batch_merge_size])\n",
        "\n",
        "        try:\n",
        "            if existing_dataset is None:\n",
        "                dataset_dict = DatasetDict({\"train\": merged_batches})\n",
        "                dataset_dict.push_to_hub(dataset_name)\n",
        "                print(f\"Uploaded initial batches {i}-{i + batch_merge_size - 1} to Hugging Face Hub as '{dataset_name}'.\")\n",
        "                existing_dataset = dataset_dict\n",
        "            else:\n",
        "                combined_dataset = DatasetDict({\n",
        "                    \"train\": concatenate_datasets([existing_dataset[\"train\"], merged_batches])\n",
        "                })\n",
        "                combined_dataset.push_to_hub(dataset_name)\n",
        "                print(f\"Appended batches {i}-{i + batch_merge_size - 1} to '{dataset_name}' on Hugging Face Hub.\")\n",
        "                existing_dataset = combined_dataset\n",
        "        except Exception as e:\n",
        "            print(f\"Error while uploading batches {i}-{i + batch_merge_size - 1} to Hugging Face Hub: {e}\")\n",
        "            raise e\n",
        "\n",
        "train_batch_size = 1024\n",
        "test_batch_size = 1024\n",
        "\n",
        "process_and_save(train_data, output_dir=\"./train_batches\", batch_size=train_batch_size, split_name=\"train\")\n",
        "process_and_save(test_data, output_dir=\"./test_batches\", batch_size=test_batch_size, split_name=\"test\")\n",
        "\n",
        "train_batches = [\n",
        "    Dataset.load_from_disk(os.path.join(\"./train_batches\", f))\n",
        "    for f in os.listdir(\"./train_batches\") if f.endswith(\".dataset\")\n",
        "]\n",
        "test_batches = [\n",
        "    Dataset.load_from_disk(os.path.join(\"./test_batches\", f))\n",
        "    for f in os.listdir(\"./test_batches\") if f.endswith(\".dataset\")\n",
        "]\n",
        "\n",
        "upload_batches_to_hub(train_batches + test_batches, \"knowledge_base_genai\", token=\" \")"
      ]
    }
  ]
}